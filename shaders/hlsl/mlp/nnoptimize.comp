#include "nnutil.h"

#define ACTIVATION_FUNCTION leakyRelu

Texture2D inputImage : register(t0);
RWTexture2D<float4> resultImage : register(u1);
RWStructuredBuffer<float> weights : register(u2);
RWStructuredBuffer<float> biases : register(u3);
RWStructuredBuffer<int> gradientWeights : register(u4);
RWStructuredBuffer<int> gradientBiases : register(u5);
RWStructuredBuffer<float> adamWeightsMeans : register(u6);
RWStructuredBuffer<float> adamWeightsVariances : register(u7);
RWStructuredBuffer<float> adamBiasesMeans : register(u8);
RWStructuredBuffer<float> adamBiasesVariances : register(u9);

StructuredBuffer<uint> neuronsPerLayer : register(t11);;
StructuredBuffer<uint> connectionDataBaseOffsets : register(t12);;
StructuredBuffer<uint> neuronDataBaseOffsets : register(t13);;

cbuffer globalinfo : register(b10) { NNData globalinfo; }

uint getNeuronDataIndex(uint layer, uint neuron)
{
    return neuronDataBaseOffsets[layer] + neuron;
}

uint getConnectionDataIndex(uint layer, uint neuronFrom, uint neuronTo)
{
    return connectionDataBaseOffsets[layer] + (neuronTo * neuronsPerLayer[layer - 1]) + neuronFrom;
}


// Source: "Positional Encoding" from "NeRF: Representing Scenes as Neural RadianceFields for View Synthesis"
// https://arxiv.org/pdf/2003.08934
// void frequencyEncoding(const float2 input, inout float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
// {
//     const int inputCount = 2;
    
//     int index = 0;
//     [unroll]
//     for (int inputIndex = 0; inputIndex < inputCount; inputIndex++)
//     {
//         const float p = PI * input[inputIndex];
//         int modifier = 1;
        
//         [unroll]
//         for (int f = 0; f < NUM_FREQUENCIES; f++)
//         {
//             const float x = modifier * p;
//             activations[index++] = sin(x);
//             activations[index++] = cos(x);
//             modifier *= 2;
//         }
//     }
// }

void encodeInput(const float2 input, inout float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
{
// #if USE_IDENTITY_ENCODING
    
    // Identity encoding passes input as it is
    activations[0] = input.x;
    activations[1] = input.y;
    
// #else // if USE_FREQUENCY_ENCODING
    
//     frequencyEncoding(input, activations);
    
// #endif
}

// =========================================================================
//   Inference
// =========================================================================

void forwardPass(float2 input, inout float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
{
    // Encode input into first layer activations
    encodeInput(input, activations);
    
    // Calculate activations for every layer, going forward through the MLP network
    [unroll]
    for (uint layer = 1; layer < LAYER_COUNT; layer++)
    {
        const uint neuronCountCurrentLayer = neuronsPerLayer[layer];
        const uint neuronCountPreviousLayer = neuronsPerLayer[layer - 1];
   
        [unroll(MAX_NEURONS_PER_LAYER)]
        for (uint neuron = 0; neuron < neuronCountCurrentLayer; neuron++)
        {
            const uint neuronDataIndex = getNeuronDataIndex(layer, neuron);
            
            // Evaluate neuron activation
            float neuronValue = biases[neuronDataIndex];
            
            // Accumulate weighted contribution from all neurons connected to this neuron in previous layer
            for (uint previousNeuron = 0; previousNeuron < neuronCountPreviousLayer; previousNeuron++)
            {
                const uint weightDataIndex = getConnectionDataIndex(layer, previousNeuron, neuron);
                const uint previousNeuronDataIndex = getNeuronDataIndex(layer - 1, previousNeuron);
                
                neuronValue += weights[weightDataIndex] * activations[previousNeuronDataIndex];
            }
            
            activations[neuronDataIndex] = ACTIVATION_FUNCTION(neuronValue);
        }
    }
}

// Source: "ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION"
// https://arxiv.org/pdf/1412.6980
float AdamOptimizer(const float gradient, RWStructuredBuffer<float> means, RWStructuredBuffer<float> variances, const uint dataIndex)
{
    // Load mean and variance
    float mean = means[dataIndex];
    float variance = variances[dataIndex];
    
    // Update mean and variance for this training step
    mean = lerp(gradient, mean, globalinfo.adamBeta1);
    variance = lerp((gradient * gradient), variance, globalinfo.adamBeta2);
    
    // Calculate weight adjustment
    const float correctedMean = mean / (1.0f - globalinfo.adamBeta1T);
    const float correctedVariance = variance / (1.0f - globalinfo.adamBeta2T);
    const float weightAdjustment = -globalinfo.learningRate * (correctedMean / (sqrt(correctedVariance) + globalinfo.adamEpsilon));
    
    // Store updated mean and variance
    means[dataIndex] = mean;
    variances[dataIndex] = variance;
    
    return weightAdjustment;
}

// Runs inference to produce image output using the current network state
[numthreads(16, 16, 1)]
void main(
	int2 groupID : SV_GroupID,
	int2 groupThreadID : SV_GroupThreadID,
	int2 LaunchIndex : SV_DispatchThreadID)
{
    const uint layer = LaunchIndex.x + 1;
    const uint neuron = LaunchIndex.y;
    if (layer >= LAYER_COUNT) return;
    
    const uint neuronCountCurrentLayer = neuronsPerLayer[layer];
    const uint neuronCountPreviousLayer = neuronsPerLayer[layer - 1];
    if (neuron >= neuronCountCurrentLayer) return;
    
    // Update bias value
    const uint neuronDataIndex = getNeuronDataIndex(layer, neuron);
    const float gradBias = unpackFloat(gradientBiases[neuronDataIndex]) * globalinfo.rcpBatchSize;
    
   // biases[neuronDataIndex] -= gradBias * globalinfo.learningRate;
   
    biases[neuronDataIndex] += AdamOptimizer(gradBias, adamBiasesMeans, adamBiasesVariances, neuronDataIndex);
    
    // Update weights leading to this neuron
    for (uint previousNeuron = 0; previousNeuron < neuronCountPreviousLayer; previousNeuron++)
    {
        const uint weightIndex = getConnectionDataIndex(layer, previousNeuron, neuron);
        const float gradWeight = unpackFloat(gradientWeights[weightIndex]) * globalinfo.rcpBatchSize;
    

        //weights[weightIndex] -= gradWeight * globalinfo.learningRate;
        weights[weightIndex] += AdamOptimizer(gradWeight, adamWeightsMeans, adamWeightsVariances, weightIndex);
    }
}