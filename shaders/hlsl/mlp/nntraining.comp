#include "nnutil.h"

#define ACTIVATION_FUNCTION leakyRelu
#define ACTIVATION_FUNCTION_DERIV leakyReluDeriv

Texture2D inputImage : register(t0);
RWTexture2D<float4> resultImage : register(u1);
RWStructuredBuffer<float> weights : register(u2);
RWStructuredBuffer<float> biases : register(u3);
RWStructuredBuffer<int> gradientWeights : register(u4);
RWStructuredBuffer<int> gradientBiases : register(u5);
RWStructuredBuffer<float> adamWeightsMeans : register(u6);
RWStructuredBuffer<float> adamWeightsVariances : register(u7);
RWStructuredBuffer<float> adamBiasesMeans : register(u8);
RWStructuredBuffer<float> adamBiasesVariances : register(u9);

StructuredBuffer<uint> neuronsPerLayer : register(t11);;
StructuredBuffer<uint> connectionDataBaseOffsets : register(t12);;
StructuredBuffer<uint> neuronDataBaseOffsets : register(t13);;

cbuffer globalinfo : register(b10) { NNData globalinfo; }

uint getNeuronDataIndex(uint layer, uint neuron)
{
    return neuronDataBaseOffsets[layer] + neuron;
}

uint getConnectionDataIndex(uint layer, uint neuronFrom, uint neuronTo)
{
    return connectionDataBaseOffsets[layer] + (neuronTo * neuronsPerLayer[layer - 1]) + neuronFrom;
}


// Source: "Positional Encoding" from "NeRF: Representing Scenes as Neural RadianceFields for View Synthesis"
// https://arxiv.org/pdf/2003.08934
// void frequencyEncoding(const float2 input, inout float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
// {
//     const int inputCount = 2;
    
//     int index = 0;
//     [unroll]
//     for (int inputIndex = 0; inputIndex < inputCount; inputIndex++)
//     {
//         const float p = PI * input[inputIndex];
//         int modifier = 1;
        
//         [unroll]
//         for (int f = 0; f < NUM_FREQUENCIES; f++)
//         {
//             const float x = modifier * p;
//             activations[index++] = sin(x);
//             activations[index++] = cos(x);
//             modifier *= 2;
//         }
//     }
// }

void encodeInput(const float2 input, inout float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
{
// #if USE_IDENTITY_ENCODING
    
    // Identity encoding passes input as it is
    activations[0] = input.x;
    activations[1] = input.y;
    
// #else // if USE_FREQUENCY_ENCODING
    
//     frequencyEncoding(input, activations);
    
// #endif
}

// =========================================================================
//   Inference
// =========================================================================

void forwardPass(float2 input, inout float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
{
    // Encode input into first layer activations
    encodeInput(input, activations);
    
    // Calculate activations for every layer, going forward through the MLP network
    [unroll]
    for (uint layer = 1; layer < LAYER_COUNT; layer++)
    {
        const uint neuronCountCurrentLayer = neuronsPerLayer[layer];
        const uint neuronCountPreviousLayer = neuronsPerLayer[layer - 1];
   
        [unroll(MAX_NEURONS_PER_LAYER)]
        for (uint neuron = 0; neuron < neuronCountCurrentLayer; neuron++)
        {
            const uint neuronDataIndex = getNeuronDataIndex(layer, neuron);
            
            // Evaluate neuron activation
            float neuronValue = biases[neuronDataIndex];
            
            // Accumulate weighted contribution from all neurons connected to this neuron in previous layer
            for (uint previousNeuron = 0; previousNeuron < neuronCountPreviousLayer; previousNeuron++)
            {
                const uint weightDataIndex = getConnectionDataIndex(layer, previousNeuron, neuron);
                const uint previousNeuronDataIndex = getNeuronDataIndex(layer - 1, previousNeuron);
                
                neuronValue += weights[weightDataIndex] * activations[previousNeuronDataIndex];
            }
            
            activations[neuronDataIndex] = ACTIVATION_FUNCTION(neuronValue);
        }
    }
}

// =========================================================================
//   Network Training
// =========================================================================

void backpropagation(float3 target, float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER])
{
    float errors[LAYER_COUNT * MAX_NEURONS_PER_LAYER];
    // Output layer derivatives
    {
        const uint neuronCountCurrentLayer = neuronsPerLayer[LAYER_COUNT - 1];
        const uint neuronCountPreviousLayer = neuronsPerLayer[LAYER_COUNT - 2];
   
        for (uint neuron = 0; neuron < neuronCountCurrentLayer; neuron++)
        {
            const uint neuronDataIndex = getNeuronDataIndex(LAYER_COUNT - 1, neuron);
            const float neuronActivation = activations[neuronDataIndex];
            const float dCost_O = 2.0f * (neuronActivation - target[neuron]);
            const float dO_Z = ACTIVATION_FUNCTION_DERIV(neuronActivation);
            const float dCost_Z = dCost_O * dO_Z;
            errors[neuronDataIndex] = dCost_Z;
            InterlockedAdd(gradientBiases[NonUniformResourceIndex(neuronDataIndex)], packFloat(dCost_Z));
            
            // Update weights
            for (uint previousNeuron = 0; previousNeuron < neuronCountPreviousLayer; previousNeuron++)
            {
                const uint previousNeuronDataIndex = getNeuronDataIndex(LAYER_COUNT - 2, previousNeuron);
                const float dCost_weight = dCost_Z * activations[previousNeuronDataIndex];
                const uint weightIndex = getConnectionDataIndex(LAYER_COUNT - 1, previousNeuron, neuron);
                InterlockedAdd(gradientWeights[NonUniformResourceIndex(weightIndex)], packFloat(dCost_weight));
            }
        }
    }
    
    // Hidden layer derivatives
    {
        [unroll(LAYER_COUNT - 2)]
        for (uint layer = LAYER_COUNT - 2; layer > 0; layer--)
        {
            const uint neuronCountCurrentLayer = neuronsPerLayer[layer];
            const uint neuronCountPreviousLayer = neuronsPerLayer[layer - 1];
            const uint neuronCountNextLayer = neuronsPerLayer[layer + 1];
   
            for (uint neuron = 0; neuron < neuronCountCurrentLayer; neuron++)
            {
                float dCost_O = 0.0f;
                for (uint nextNeuron = 0; nextNeuron < neuronCountNextLayer; nextNeuron++)
                {
                    const uint weightIndex = getConnectionDataIndex(layer + 1, neuron, nextNeuron);
                    const uint nextNeuronDataIndex = getNeuronDataIndex(layer + 1, nextNeuron);
                    dCost_O += (errors[nextNeuronDataIndex] * weights[NonUniformResourceIndex(weightIndex)]);
                }
                
                const uint neuronDataIndex = getNeuronDataIndex(layer, neuron);
                const float neuronActivation = activations[neuronDataIndex];
                const float dO_Z = ACTIVATION_FUNCTION_DERIV(neuronActivation);
                const float dCost_Z = dCost_O * dO_Z;
                errors[neuronDataIndex] = dCost_Z;
                InterlockedAdd(gradientBiases[NonUniformResourceIndex(neuronDataIndex)], packFloat(dCost_Z));

                // Update weights
                for (uint previousNeuron = 0; previousNeuron < neuronCountPreviousLayer; previousNeuron++)
                {
                    const uint previousNeuronDataIndex = getNeuronDataIndex(layer - 1, previousNeuron);
                    const float dCost_weight = dCost_Z * activations[previousNeuronDataIndex];
                    const uint weightIndex = getConnectionDataIndex(layer, previousNeuron, neuron);
                    InterlockedAdd(gradientWeights[NonUniformResourceIndex(weightIndex)], packFloat(dCost_weight));
                }
            }
        }
    }
}


// Runs inference to produce image output using the current network state
[numthreads(8, 1, 1)]
void main(
	int2 groupID : SV_GroupID,
	int2 groupThreadID : SV_GroupThreadID,
	int2 LaunchIndex : SV_DispatchThreadID)
{
    if (LaunchIndex.x >= globalinfo.batchSize || LaunchIndex.y > 0) return;
    
    // Initialize random numbers generator
    uint rng = initRNG(LaunchIndex, uint2(1, 1), globalinfo.frameNumber);
    
    // Generate a random input (UV coordinates in the image)
    const float2 uvs = float2(rand(rng), rand(rng));
    
    // Load target value to learn for this input from reference image
    const float3 target = inputImage[uvs * float2(globalinfo.outputWidth - 1, globalinfo.outputHeight - 1)].rgb;
    
    // First run forward pass to evaluate network activations for given input
    float activations[LAYER_COUNT * MAX_NEURONS_PER_LAYER];
    forwardPass(uvs, activations);
    
    // Run backpropagation on current network state
    backpropagation(target, activations);
}